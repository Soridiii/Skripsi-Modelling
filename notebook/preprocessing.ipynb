{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83f0ab3",
   "metadata": {},
   "source": [
    "### Import Library dan CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8901c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from indoNLP.preprocessing import replace_word_elongation, replace_slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b05751cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bisa-bisanya mark gak calling gue /marah</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ingett wkwk. iyaa dog lalu kita bertiga , sama...</td>\n",
       "      <td>Joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bersyukur sekali karena berkatmu aku juga baha...</td>\n",
       "      <td>Joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kalian ada yang pernah pengen banget kiss pipi...</td>\n",
       "      <td>Love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kok bisa bisanya kaget, bukanya ente yg ngawas...</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Label\n",
       "0           bisa-bisanya mark gak calling gue /marah  Anger\n",
       "1  ingett wkwk. iyaa dog lalu kita bertiga , sama...    Joy\n",
       "2  bersyukur sekali karena berkatmu aku juga baha...    Joy\n",
       "3  kalian ada yang pernah pengen banget kiss pipi...   Love\n",
       "4  kok bisa bisanya kaget, bukanya ente yg ngawas...  Anger"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angerData = pd.read_csv('../data/raw_data/AngerData.csv', delimiter='\\t')\n",
    "fearData = pd.read_csv('../data/raw_data/FearData.csv', delimiter='\\t')\n",
    "joyData = pd.read_csv('../data/raw_data/JoyData.csv', delimiter='\\t')\n",
    "loveData = pd.read_csv('../data/raw_data/LoveData.csv', delimiter='\\t')\n",
    "sadData = pd.read_csv('../data/raw_data/SadData.csv', delimiter='\\t')\n",
    "    \n",
    "#gabungkan data\n",
    "df1 = pd.concat([angerData, fearData, joyData, loveData, sadData], ignore_index=True)\n",
    "df1 = df1.sample(frac=1).reset_index(drop=True) #shuffle data\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00745b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "Gembira    1271\n",
      "Marah      1049\n",
      "Sedih      1003\n",
      "Takut       911\n",
      "Cinta       760\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Mengubag nama label menjadi bahasa Indonesia\n",
    "label_mapping = {\n",
    "    'Anger': 'Marah',\n",
    "    'anger': 'Marah',\n",
    "    'Fear': 'Takut',\n",
    "    'fear': 'Takut',\n",
    "    'happy': 'Gembira',\n",
    "    'Joy': 'Gembira',\n",
    "    'love': 'Cinta',\n",
    "    'Love': 'Cinta',\n",
    "    'sadness': 'Sedih',\n",
    "    'Sad': 'Sedih'\n",
    "}\n",
    "df1['Label'] = df1['Label'].map(label_mapping)\n",
    "print(df1['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e63f55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "angerData = pd.read_csv('../data/new_data/marah.csv')\n",
    "fearData = pd.read_csv('../data/new_data/takut.csv')\n",
    "joyData = pd.read_csv('../data/new_data/gembira.csv')\n",
    "loveData = pd.read_csv('../data/new_data/cinta.csv')\n",
    "sadData = pd.read_csv('../data/new_data/sedih.csv')\n",
    "#gabungkan data\n",
    "df2 = pd.concat([angerData, fearData, joyData, loveData, sadData], ignore_index=True)\n",
    "df2 = df2.sample(frac=1).reset_index(drop=True) #shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02b58e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.read_csv('../data/raw_data/emotion_dataset.csv')\n",
    "# #change column name\n",
    "# df2 = df2.rename(columns={'label':'Label', 'tweet':'Tweet'})\n",
    "\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee1ccce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gue takut kehilangan hal kecil</td>\n",
       "      <td>Takut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ga ada cahaya buat dituju</td>\n",
       "      <td>Sedih</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rasanya like semuanya sinkron</td>\n",
       "      <td>Gembira</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yaallah mika angkasa w kaget gue kira cm ciuma...</td>\n",
       "      <td>Takut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gue cemas sejak tadi pagi</td>\n",
       "      <td>Takut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet    Label\n",
       "0                     gue takut kehilangan hal kecil    Takut\n",
       "1                          ga ada cahaya buat dituju    Sedih\n",
       "2                      rasanya like semuanya sinkron  Gembira\n",
       "3  yaallah mika angkasa w kaget gue kira cm ciuma...    Takut\n",
       "4                          gue cemas sejak tadi pagi    Takut"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True) #shuffle data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3926993e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "Gembira    2423\n",
      "Marah      2055\n",
      "Sedih      2037\n",
      "Takut      1977\n",
      "Cinta      1685\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cd74652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet    gue cemas sejak tadi pagi\n",
      "Label                        Takut\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(df.iloc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c37853a",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccbe2a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1305 entries from kamus.csv\n"
     ]
    }
   ],
   "source": [
    "def cleanDataframe(df):\n",
    "    print('Missing Values:', df.isnull().sum())\n",
    "    print('Duplicates:', df.duplicated().sum())\n",
    "\n",
    "    df_cleaned = df.dropna(subset='Tweet')\n",
    "    df_cleaned = df_cleaned.drop_duplicates(keep='first')\n",
    "\n",
    "    print('\\nMissing values after cleaning:', df_cleaned.isnull().sum())\n",
    "    print('Duplicates after cleaning:', df_cleaned.duplicated().sum())\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cleaningText(text):\n",
    "    # Membersihkan tanda tanda sisa medsos\n",
    "    text = re.sub(r'\\[USERNAME\\]', '', text)\n",
    "    text = re.sub(r'\\[URL\\]', '', text)\n",
    "    text = re.sub(r'\\[SENSITIVE-NO\\]', '', text)\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Menghapus Mention\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text)  # Menghapus Hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text)  # menghapus RT\n",
    "    text = re.sub(r\"http\\S+\", '', text)  # menghapus link\n",
    "\n",
    "    # Pembersihan Karakter\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))# Mengganti tanda baca dengan spasi (alih-alih menghapus)\n",
    "    text = re.sub(r'\\d+', '', text)  # Menghapus angka\n",
    "    text = text.replace('\\n', ' ')  # Mengganti garis baru dengan spasi\n",
    "    return text\n",
    "\n",
    "\n",
    "kamusCleaning = pd.read_csv('../data/kamus.csv', encoding='utf-8',\n",
    "                            delimiter=',', header=None, names=['slang', 'formal'])\n",
    "\n",
    "cleaningDict = {}\n",
    "for slang, formal in zip(kamusCleaning.slang, kamusCleaning.formal):\n",
    "    if pd.notna(slang) and pd.notna(formal):\n",
    "        slang_lower = str(slang).lower().strip()\n",
    "        formal_clean = str(formal).strip()\n",
    "        cleaningDict[slang_lower] = formal_clean\n",
    "        \n",
    "print(f\"✅ Loaded {len(cleaningDict)} entries from kamus.csv\")\n",
    "\n",
    "def applyCleaningDict(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return text\n",
    "\n",
    "    # Pattern untuk match whole words (case insensitive)\n",
    "    words = text.split()\n",
    "    cleaned_words = []\n",
    "\n",
    "    for word in words:\n",
    "        # Clean word (lowercase untuk matching)\n",
    "        clean_word = word.lower().strip()\n",
    "\n",
    "        # Check jika word ada di dictionary\n",
    "        if clean_word in cleaningDict:\n",
    "            cleaned_words.append(cleaningDict[clean_word])\n",
    "        else:\n",
    "            cleaned_words.append(word)\n",
    "\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    return cleaned_text\n",
    "\n",
    "def normalizeText(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    if not text.strip():\n",
    "        return text.strip()\n",
    "\n",
    "    # 1. Lowercase pertama\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Handle character elongation\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "    if 'replace_word_elongation' in globals():\n",
    "        text = replace_word_elongation(text)\n",
    "\n",
    "    # 3. Replace slang words (dari multiple sources)\n",
    "    if 'replace_slang' in globals():\n",
    "        text = replace_slang(text)\n",
    "    text = applyCleaningDict(text)\n",
    "\n",
    "    # 4. Final cleanup\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de2e8775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet    benar guys, nasi goreng.,simple, cepat, dan en...\n",
      "Label                                              Gembira\n",
      "Name: 69, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ceeef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: Tweet    0\n",
      "Label    0\n",
      "dtype: int64\n",
      "Duplicates: 102\n",
      "\n",
      "Missing values after cleaning: Tweet    0\n",
      "Label    0\n",
      "dtype: int64\n",
      "Duplicates after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = cleanDataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5bfe783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet    sedih banget temen gue \n",
      "Label                      Sedih\n",
      "Name: 61, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_cleaned['Tweet'] = df_cleaned['Tweet'].apply(cleaningText)\n",
    "# Print 1 sample not truncated\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(df_cleaned.iloc[61])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2f8e909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumlah kata unik sebelum normalisasi: 19170\n"
     ]
    }
   ],
   "source": [
    "all_text = \"\".join(df_cleaned[\"Tweet\"].astype(str))\n",
    "text_length = all_text.split()\n",
    "unique_text = set(text_length)\n",
    "print('jumlah kata unik sebelum normalisasi:', len(unique_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64d812fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet    sedih sekali teman saya\n",
      "Label                      Sedih\n",
      "Name: 61, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_normalized = df_cleaned.copy()\n",
    "df_normalized['Tweet'] = df_normalized['Tweet'].apply(normalizeText)\n",
    "# Print 1 sample not truncated\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(df_normalized.iloc[61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81e02988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumlah kata unik setelah normalisasi: 18107\n"
     ]
    }
   ],
   "source": [
    "all_text_normalized = \"\".join(df_normalized[\"Tweet\"].astype(str))\n",
    "text_length_normalized = all_text_normalized.split()\n",
    "unique_text_normalized = set(text_length_normalized)\n",
    "print('jumlah kata unik setelah normalisasi:', len(unique_text_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f21c7ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "Gembira    2402\n",
      "Marah      2046\n",
      "Sedih      2007\n",
      "Takut      1953\n",
      "Cinta      1667\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae930ad1",
   "metadata": {},
   "source": [
    "### Export Menjadi CSV Baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b39f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "df_normalized.to_csv('../data/data_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
